{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "German Traffic Sign Dataset merupakan benchmark dataset untuk klasifikasi gambar multi kelas. Terdapat satu rambu lalu lintas pada masing-masing gambar, sehingga dataset ini disebut juga sebagai single-image atau gambar tunggal. Dataset ini pertama kali diluncurkan dalam perlombaan (challenge) yang diselenggarakan oleh the International Joint Conference on Neural Networks (IJCNN) pada tahun 2011. \n",
    "\n",
    "Dataset ini memiliki komponen sebagai berikut:\n",
    "\n",
    "Data untuk permasalahan klasifikasi gambar tunggal dengan banyak kelas.\n",
    "Terdiri dari 43 kelas, artinya ada 43 jenis rambu lalu lintas.\n",
    "Memiliki total lebih dari 50.000 gambar .\n",
    "Data diambil dari foto rambu yang sebenarnya (bukan sintetis).\n",
    "Data yang akan kita gunakan dalam bentuk file pickle yang dibuat dengan modul Python bernama pickle. Ia digunakan untuk mengubah objek Python menjadi representasi byte untuk disimpan pada storage atau dipindahkan melalui jaringan. Hal ini memungkinkan objek untuk disimpan atau ditransmisikan dengan mudah tanpa mengubah data ke format lain terlebih dahulu.\n",
    "https://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, Dense, Flatten, Dropout, MaxPool2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "training_file = \"/content/drive/MyDrive/Dataset/German Traffic Sign/train.p\"\n",
    "testing_file = \"/content/drive/MyDrive/Dataset/German Traffic Sign/test.p\"   \n",
    " \n",
    "# Open and load the training file \n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    " \n",
    "# Open and load the testing file\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ekplorasi Data\n",
    "Tahapan eksplorasi data yang pertama adalah membaca file signnames.csv. File ini berisi id dan nama kelas dataset traffic sign. Mari implementasikan kode berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Buat pandas dataframe untuk load data csv\n",
    "## File csv ini berupa ClassId dan SignName\n",
    " \n",
    "sign_name_df = pd.read_csv('/content/drive/MyDrive/Dataset/German Traffic Sign/signnames.csv')\n",
    "SIGN_NAMES = sign_name_df.SignName.values\n",
    "sign_name_df.set_index('ClassId', inplace=True)\n",
    "sign_name_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisikan fitur dan label untuk data training\n",
    "X, y = train['features'], train['labels']\n",
    " \n",
    "# Mengubah lists menjadi numpy arrays\n",
    "data = np.array(X)\n",
    "labels = np.array(y)\n",
    "print(data.shape, labels.shape)\n",
    " \n",
    "# Definisikan fitur dan label untuk data testing\n",
    "X_test, y_test = test['features'], test['labels']\n",
    " \n",
    "# Mengubah lists menjadi numpy arrays\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = np.unique(y_train).size\n",
    "def hist_data(y_data, title=None, ax=None, **kwargs):\n",
    "    if not ax :\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "    ax.hist(y_data, np.arange(-0.5, n_labels+1.5), stacked=True, **kwargs)\n",
    "    ax.set_xlim(-0.5, n_labels-0.5)\n",
    "    if 'label' in kwargs : ax.legend()\n",
    "    if title : ax.set_title(title)\n",
    "        \n",
    "fig,ax = plt.subplots(1,3, figsize=(20,5))\n",
    "hist_data(y_train, title='Distribusi kelas pada data training', ax=ax[0])\n",
    "hist_data(y_val, title='Distribusi kelas pada data validasi', ax=ax[1], color='black')\n",
    "hist_data(y_test, title='Distribusi kelas pada data test', ax=ax[2], color='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Data](distribusi_traffic_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi label dengan teknik one hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    " \n",
    "y_train = to_categorical(y_train, 43)\n",
    "y_val = to_categorical(y_val, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy') > 0.96):\n",
    "      print(\"\\nAkurasi telah mencapai >96%. Stop training!\")\n",
    "      self.model.stop_training = True\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Modelling Architecture](modelling.png)\n",
    "\n",
    "Mari kita ingat kembali sebuah arsitektur Convolutional Network sederhana yang memiliki arsitektur lapisan (layer) Input-Convolutional-RELU-Pooling-FC. Berikut uraiannya:\n",
    "\n",
    "1. apisan Input akan menampung gambar sebagai array 3D dari nilai piksel.\n",
    "\n",
    "2. Lapisan Convolutional (konvolusi) akan menghitung perkalian (dot product) antara kernel dan sub-array dari gambar input yang berukuran sama dengan kernel. Sebagai contoh pada gambar berikut kita menggunakan kernel berukuran 3x3 dan melakukan proses perkalian antara kernel dan sub-array pada gambar masukan.\n",
    "\n",
    "![Perkalian Kernel pada Konvolusi](perkalian_konvolusi.gif)\n",
    "\n",
    "Matriks kernel akan melintasi setiap piksel pada gambar secara berurutan dari kiri ke kanan dan atas ke bawah. Ia menghitung nilai output untuk setiap piksel yang dilintasi kernel dengan teknik perkalian matriks dot product. Kemudian, semua nilai yang dihasilkan dari proses perkalian dot product ini akan menjadi nilai piksel tunggal dari gambar keluaran. Proses ini diulang sampai seluruh gambar input terlewati oleh kernel.\n",
    "\n",
    "    RELU atau Rectified Linear Activation Unit merupakan fungsi aktivasi linear untuk jaringan saraf tiruan. Ia telah menjadi fungsi aktivasi default karena model yang menggunakannya mencapai kinerja lebih baik dengan cepat.\n",
    "\n",
    "    Ingatlah, fungsi aktivasi digunakan untuk memetakan hasil penjumlahan antara bobot dengan masukan (disebut sebagai weighted sum) menjadi nilai tertentu. Fungsi ini memungkinkan perceptron dapat menyesuaikan pola untuk data non linear.\n",
    "\n",
    "    Nah, jika weighted sum ini bernilai positif, RELU akan mengembalikan nilainya secara langsung. Sebaliknya, jika nilainya negatif, RELU akan menghasilkan nilai nol. RELU didefinisikan sebagai:\n",
    "\n",
    "    f(x) = max(0, x), dengan x adalah jaringan saraf masukan (weighted sum).\n",
    "\n",
    "\n",
    "3. Lapisan Pooling akan melakukan downsampling pada shape suatu gambar sehingga mengakibatkan pengurangan dimensi gambar. Tujuannya adalah agar data komputasi yang dibutuhkan untuk memproses citra menjadi berkurang.\n",
    "\n",
    "Pooling layer terdiri dari dua jenis, average pooling dan max pooling. Pada max pooling, setiap area dengan luas piksel tertentu akan diambil satu buah piksel yang memiliki nilai tertinggi. Sementara pada average pooling, nilai yang diambil adalah nilai rata-rata dari suatu area kernel. Untuk lebih jelasnya, mari kita perhatikan gambar berikut.\n",
    "\n",
    "![Pooling](pooling.png)\n",
    "\n",
    "    Lapisan pooling juga bertindak sebagai pengendali noise (gangguan). Namun, max pooling memiliki kinerja lebih baik dibanding dengan average pooling. Oleh karena itu, max pooling lebih sering digunakan pada proses training CNN. Kita juga akan menggunakannya dalam proyek ini.\n",
    "\n",
    "4. Lapisan FC (Fully-Connected) akan menghitung skor kelas untuk setiap kategori klasifikasi. Lapisan ini sama dengan jaringan saraf di mana setiap neuron terhubung ke semua neuron pada lapisan secara berurutan. Output akhir dihitung menggunakan softmax yang memberikan probabilitas setiap kelas untuk fitur yang diberikan.\n",
    "\n",
    "Pada proyek ini, kita menggunakan arsitektur 2 convolutional layer dan 1 fully connected layer. Untuk lebih jelasnya, mari kita bahas setiap layernya.\n",
    "\n",
    "Pertama, inisialisasi model Sequential dan tentukan input shape untuk model. Set layer ini menggunakan kernel berukuran 5x5 untuk mempelajari fitur pada data traffic sign. Input masukan kita sesuaikan dengan shape X_train yaitu sebuah gambar RGB berukuran 32 pixels (32, 32, 3). Fungsi aktivasi yang akan kita gunakan adalah RELU, seperti yang telah dijelaskan sebelumnya. Max Pooling layer juga kita tambahkan untuk mengurangi dimensi.\n",
    "\n",
    "Di sini, kita juga menerapkan dropout. Dropout merupakan salah satu hyperparameter yang bertujuan untuk mengurangi overfitting. Ia bekerja dengan memutus atau menjatuhkan (drop) unit selama proses training neural network sehingga layer atau lapisannya menjadi tidak aktif. Berdasarkan dokumentasi dari Keras, argumen rate untuk dropout adalah tipe data float antara 0 dan 1, yang merupakan bagian dari unit yang akan di-drop. \n",
    "\n",
    "Jika kita set dropout rate = 0.25, artinya, kita akan melepas 25% dari layer dan membuatnya menjadi tidak aktif selama proses training.\n",
    "\n",
    "![set layer](set_layer.gif)\n",
    "\n",
    "Berikut set layer kita yang pertama. Pada arsitektur ini, set parameter layer sebagai berikut:\n",
    "- Ukuran filter untuk proses konvolusi=32\n",
    "- Ukuran kernel=(5,5)\n",
    "- Fungsi aktivasi RELU\n",
    "- Pooling yang kita gunakan adalah Maxpool dengan ukuran 2,2\n",
    "- Dropout rate sebesar 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(43, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "epochs = 25\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=epochs, validation_data=(X_val, y_val), callbacks=[callbacks])\n",
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting graphs for accuracy \n",
    "plt.figure(0)\n",
    "plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    " \n",
    "# Plotting graphs for loss\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
